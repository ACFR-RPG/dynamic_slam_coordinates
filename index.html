<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description"
    content="Evaluation and comparison of different formulations for Dynamic SLAM">
  <meta property="og:title" content="The Importance of Coordinate Frames in Dyamic SLAM" />
  <meta property="og:description" content="Exploration and evalutation of factor-graph formulations for Dynamic SLAM." />
  <meta property="og:url" content="https://acfr-rpg.github.io/dynamic_slam_coordinates/" />
  <meta property="og:image" content="https://acfr-rpg.github.io/dynamic_slam_coordinates/static/images/frontpage_frames.svg" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />

  <meta name="twitter:title" content="The Importance of Coordinate Frames in Dynamic SLAM">
  <meta name="twitter:description" content="Exploration and evalutation of factor-graph formulations for Dynamic SLAM.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="https://acfr-rpg.github.io/dynamic_slam_coordinates/static/images/frontpage_frames.svg">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="SLAM, Dynamic SLAM, factor graphs">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>The Importance of Coordinate Frames in Dyamic SLAM</title>
  <link rel="icon" type="image/x-icon" href="static/images/acfr_rpg_logo.png">

  <link rel="stylesheet" href="static/css/bootstrap.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script> -->
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bootstrap.bundle.min.js"></script>
</head>

<body>
  <div class="container">
    <!-- Title -->
    <h1 class="pt-5 title">The Importance of Coordinate Frames in Dyamic SLAM</h1>
    <div class="d-flex flex-row justify-content-center">
      Jesse Morris, Yiduo Wang, Viorela Ila
    </div>

    <div class="w-100 d-flex flex-row justify-content-center mt-4 gap-2">
      <!-- Paper PDF -->
      <a href="https://ieeexplore.ieee.org/abstract/document/10610840" target="_blank" class="btn btn-dark" role="button">
        <span class="icon">
          <i class="ai ai-ieee"></i>
        </span>
        <span>Paper</span>
      </a>

      <!-- Paper PDF -->
      <a href="https://arxiv.org/abs/2312.04031" target="_blank" class="btn btn-dark" role="button">
        <span class="icon">
          <i class="ai ai-arxiv"></i>
        </span>
        <span>Arxiv</span>
      </a>

      <!-- Code -->
      <a href="https://github.com/ACFR-RPG/dynamic_slam_coordinates" target="_blank" class="btn btn-dark" role="button">
        <span class="icon">
          <i class="fab fa-github"></i>
        </span>
        <span>Code</span>
      </a>

    </div>

    <!-- Teaser -->


    <!-- TL;DR
    <h2>TL;DR</h2>
    <div class="alert alert-primary tldr mb-4">
      pixelSplat infers a 3D Gaussian scene from two input views in a single forward pass.
    </div> -->

    <!-- Abstract -->
    <h2>Abstract</h2>
    <p class="mb-4">
      Most Simultaneous localisation and mapping (SLAM) systems have traditionally assumed a static world, which does not align with real-world scenarios. 
To enable robots to safely navigate and plan in dynamic environments, it is essential to employ representations capable of handling moving objects. 
Dynamic SLAM is an emerging field in SLAM research as it improves the overall system accuracy while providing additional estimation of object motions. 
State-of-the-art literature informs two main formulations for Dynamic SLAM, representing dynamic object points in either the world or object coordinate frame. 
While expressing object points in their local reference frame may seem intuitive, it does not necessarily lead to the most accurate and robust solutions. 
This paper conducts and presents a thorough analysis of various Dynamic SLAM formulations, identifying the best approach to address the problem. 
To this end, we introduce a front-end agnostic framework using GTSAM that can be used to evaluate various Dynamic SLAM formulations.
    </p>

    <img src="static/images/graphicAbstract.svg" class="img-fluid w-100 mt-2 mb-3" alt="point clouds and depth maps" />
    <!-- <h2>Comparison vs. Baselines</h2> -->
    <p>Our paper compares and evaluates two formulations for Dynamic SLAM, where we joinly optimize for camera poses, object motions/poses as well as static and dynamic 
      points using factor graphs. The first formulation represents dynamic points in the object (or local) frame and is an intuiative approach as it easy to represent a rigid body 
      using points that are static with-respect-to their local body frame. The second formulation represents dynamic points in the world frame which allows direct 3D point measurements to model the object motion in common reference frame.
      Our work addresses and analyses each approach in terms of estimation accuracy and the systems behaviour during optimization and conclues that the world-centric formulation is the better formulation.
    </p>
    <ul>
      <!-- <li><a href="https://yilundu.github.io/wide_baseline/">The Method of Du et al.</a>: A light field renderer
        designed for wide-baseline novel view synthesis.</li>
      <li><a href="https://mohammedsuhail.net/gen_patch_neural_rendering/">GPNR</a>: A light field transformer which
        struggles with only two input views.</li>
      <li><a href="https://alexyu.net/pixelnerf/">pixelNeRF</a>: A well-known NeRF-based approach which struggles on
        scene-scale datasets because it does not handle scale ambiguity.</li> -->
    </ul>

    <div class="w-100 my-4">
      <h1 class="pt-5 title">Presentation</h1>
        <video class="w-100 d-block" autoplay controls muted loop>
          <source src="static/videos/ICRA2024_1643_presentation.mp4" type="video/mp4">
        </video>
      </div>

  <div class="w-100 my-4">
    <h1 class="pt-5 title">Supplementary Video</h1>
      <video class="w-100 d-block" autoplay controls muted loop>
        <source src="static/videos/ICRA2024_4_supplementary.mp4" type="video/mp4">
      </video>
    </div>

    <!-- Comparisons -->
    <!-- <h2>Comparison vs. Baselines</h2>
    <p>We compare our method against the following baselines:</p>
    <ul>
      <li><a href="https://yilundu.github.io/wide_baseline/">The Method of Du et al.</a>: A light field renderer
        designed for wide-baseline novel view synthesis.</li>
      <li><a href="https://mohammedsuhail.net/gen_patch_neural_rendering/">GPNR</a>: A light field transformer which
        struggles with only two input views.</li>
      <li><a href="https://alexyu.net/pixelnerf/">pixelNeRF</a>: A well-known NeRF-based approach which struggles on
        scene-scale datasets because it does not handle scale ambiguity.</li>
    </ul>

    <h3>ACID Dataset</h3>
    <img src="static/images/comparison_acid.svg" class="img-fluid w-100 mt-2 mb-3" alt="comparison on ACID dataset" />
    <div class="border w-100 mb-4">
      <video class="w-100 d-block" autoplay controls muted loop>
        <source src="static/videos/output_acid.mp4" type="video/mp4">
      </video>
    </div>

    <h3>Real Estate 10k Dataset</h3>
    <img src="static/images/comparison_re10k.svg" class="img-fluid w-100 mt-2 mb-3" alt="comparison on ACID dataset" />
    <div class="border w-100 mb-4">
      <video class="w-100 d-block" autoplay controls muted loop>
        <source src="static/videos/output_re10k.mp4" type="video/mp4">
      </video>
    </div> -->

    <!-- Point Clouds -->
    <!-- <h2>3D Gaussian Point Clouds</h2>
    <p>Because pixelSplat infers a set of 3D Gaussians, we can visualize these Gaussians and render them to produce
      depth maps. Since the Real Estate 10k and ACID datasets contain many areas with ambiguous depth (e.g., large,
      textureless surfaces like interior walls), we fine-tune pixelSplat for 50,000 iterations using a depth regularizer
      before exporting 3D Gaussian point clouds.</p>
    <img src="static/images/point_clouds.svg" class="img-fluid w-100 mt-2 mb-3" alt="point clouds and depth maps" /> -->

    <!-- <p>Use the interactive 3D viewer below to explore a set of 3D Gaussians generated by pixelSplat. However, note that the renderings produced by the interactive Spline viewer do not exactly match those produced by the original CUDA-based implementation of 3D Gaussian splatting.</p>
    <iframe src='https://my.spline.design/untitled-56829706657b783ffda8a7682085e706/' frameborder='0' width='100%' height='400px'></iframe> -->

    <!-- Footer -->
    <footer class="border-top mt-5 py-4">
      This page's code uses elements from this <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
        target="_blank">Academic Project Page
        Template</a>.
    </footer>
  </div>
</body>

</html>
